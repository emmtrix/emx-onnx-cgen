/*
 * Generated by emmtrix ONNX-to-C Code Generator (emx-onnx-cgen)
 *
 * Codegen settings:
 *   emit_testbench: False
 *   restrict_arrays: True
 *   fp32_accumulation_strategy: simple
 *   fp16_accumulation_strategy: fp32
 *   large_temp_threshold: 1024
 *   large_weight_threshold: 102400
 * Model checksum (sha256): ef7ad6a1649c61dd99010deea4a13d38923197f959cbda921d7dc573fc29b353
 * Model name: model
 * Graph name: softmax_cross_entropy_loss_graph
 * Inputs: 2 Outputs: 2 Nodes: 1 Initializers: 0
 * IR version: 7
 * Model version: n/a
 * Domain: n/a
 * Producer: emx-onnx-cgen (version: n/a)
 * Opset imports: ai.onnx=13
 * Description:
 *   n/a
 * Graph description:
 *   n/a
 * Metadata:
 *   n/a
 */

#include <stdint.h>
#include <math.h>
#include <float.h>

#ifndef idx_t
#define idx_t int32_t
#endif
#ifndef EMX_UNUSED
#if defined(__GNUC__) || defined(__clang__)
#define EMX_UNUSED __attribute__((unused))
#else
#define EMX_UNUSED
#endif
#endif
#ifndef EMX_STRING_MAX_LEN
#define EMX_STRING_MAX_LEN 256
#endif

static inline float ref_scalar_f32_fmax(float a, float b) {
    return fmaxf(a, b);
}

/*
 * Node 0:
 * OpType: SoftmaxCrossEntropyLoss
 * Name: n/a
 * Inputs: scores, labels
 * Outputs: loss, log_prob
 * Attrs:
 *   reduction: mean
 */
static inline void node0_softmaxcrossentropyloss(const float input0[2][3], const int64_t target[2], float output[1], float log_prob[2][3]) {
    const float *input_flat = (const float *)input0;
    const int64_t *target_flat = (const int64_t *)target;
    float *output_flat = (float *)output;
    float *log_prob_flat = (float *)log_prob;
    const idx_t n = 2;
    const idx_t c = 3;
    const idx_t d = 1;
    float loss_sum = 0.0f;
    for (idx_t n_idx = 0; n_idx < n; ++n_idx) {
        for (idx_t d_idx = 0; d_idx < d; ++d_idx) {
            idx_t target_index = n_idx * d + d_idx;
            int64_t target_value = target_flat[target_index];
            idx_t class_index = (idx_t)target_value;
            idx_t base = (n_idx * c * d) + d_idx;
            float max_value = (float)input_flat[base];
            for (idx_t c_idx = 1; c_idx < c; ++c_idx) {
                float value = (float)input_flat[base + c_idx * d];
                max_value = ref_scalar_f32_fmax(max_value, value);
            }
            float sum = 0.0f;
            for (idx_t c_idx = 0; c_idx < c; ++c_idx) {
                float value = (float)input_flat[base + c_idx * d] - max_value;
                sum += expf(value);
            }
            float logsum = logf(sum);
            float loss_value = 0.0f;
            for (idx_t c_idx = 0; c_idx < c; ++c_idx) {
                float log_prob_value = (float)input_flat[base + c_idx * d] - max_value - logsum;
                log_prob_flat[base + c_idx * d] = (float)log_prob_value;
                if (c_idx == class_index) {
                    loss_value = -log_prob_value;
                }
            }
            loss_sum += loss_value;
        }
    }
    output_flat[0] = loss_sum / (2 * 1);
}

_Bool model_load(const char *path) {
    (void)path;
    return 1;
}

void model(const float scores[restrict 2][3], const int64_t labels[restrict 2], float loss[restrict 1], float log_prob[restrict 2][3]) {
    node0_softmaxcrossentropyloss(scores, labels, loss, log_prob);
}
